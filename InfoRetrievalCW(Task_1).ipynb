{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "IfldhjiaUSls"
      },
      "outputs": [],
      "source": [
        "#Building a Vertical Search Engine for 7071CEM - Information Retrieval Coursework\n",
        "#Importing the relevant crawling libraries\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Building the Crawler\n",
        "pubdata = []\n",
        "Ptitle = []\n",
        "\n",
        "for i in range(0, 5):\n",
        "  url = f\"https://pureportal.coventry.ac.uk/en/organisations/research-centre-for-computational-science-and-mathematical-modell/publications/?page={i}\"\n",
        "  page = requests.get(url)\n",
        "  time.sleep(5)\n",
        "  soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "  whole = soup.find_all(\"div\", class_=\"result-container\")\n",
        "\n",
        "  for info in whole:\n",
        "\n",
        "    title = info.find(\"h3\", class_=\"title\")\n",
        "    for link in title:\n",
        "      pubtitle = link.text\n",
        "      Ptitle.append(pubtitle)\n",
        "      publink = link[\"href\"]\n",
        "\n",
        "    authors = info.find_all(\"a\", class_ = \"link person\")\n",
        "    for authinfo in authors:\n",
        "      authname = authinfo.text\n",
        "      authlink = authinfo[\"href\"]\n",
        "      \n",
        "    date = info.find(\"span\", class_=\"date\").text\n",
        "   \n",
        "    pubdata.append([[pubtitle], [publink], [authname], [authlink], [date]])\n",
        "    \n",
        "print(len(pubdata))\n",
        "print(pubdata)\n",
        "#type(pubtitle) #to output the variable type which is str (string)"
      ],
      "metadata": {
        "id": "AN18oPd0WYwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compiling the data into a dataframe to display and export to csv\n",
        "\n",
        "table = pd.DataFrame(pubdata, columns=[\"Title\", \"Publication Link\", \"Authors\", \"Pureportal Link\", \"Publication Date\"])\n",
        "table\n",
        "\n",
        "#table.to_csv(\"InfoRetrievalCW.csv\") #to export to csv\n",
        "#The csv file has been uploaded to a OneDrive folder and the link will be provided in the report"
      ],
      "metadata": {
        "id": "PW6nneQd6Du5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#List of all Publication Titles as crawled from site.\n",
        "print(len(Ptitle))\n",
        "\n",
        "#Datatype of Ptitle\n",
        "type(Ptitle)"
      ],
      "metadata": {
        "id": "iAQMFC8b8m71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing Libraries for preprocessing and cleaning\n",
        "import nltk\n",
        "import string\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import defaultdict\n",
        "SW = stopwords.words(\"english\")\n",
        "Lstem = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "V8lQPvXJ4fBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A document from the corpus that contains punctuation \n",
        "print(Ptitle[8])"
      ],
      "metadata": {
        "id": "ppWY_E431TrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing Punctuation\n",
        "Pub_Title = [\"\".join(punc for punc in sent\n",
        "                     if punc not in string.punctuation)\n",
        "            for sent in Ptitle]\n",
        "\n",
        "print(Pub_Title[8])\n",
        "len(Pub_Title)"
      ],
      "metadata": {
        "id": "Sa0lpXtmMuN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocessing the data and building the inverted index\n",
        "invert_indx = defaultdict(set)\n",
        "\n",
        "for docxid, c in enumerate(Pub_Title):\n",
        "  for word in word_tokenize(c):\n",
        "    word_lower = word.lower()\n",
        "    if word_lower not in SW:\n",
        "      word_stem = Lstem.lemmatize(word_lower)\n",
        "      invert_indx[word_stem].add(docxid)\n",
        "\n",
        "print(len(invert_indx)) #outputs the total number of keywords\n",
        "print(invert_indx) #displays the inverted index for every word in our corpus"
      ],
      "metadata": {
        "id": "CKSk0NAKb-zM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This section of code does 3 things:\n",
        "#Defines a function called query_processor that accepts a value \"query\"\n",
        "#Preprocesses the query to match the inverted index format\n",
        "#Returns the index values if the \"query\" matches any word in our index set() and an empty set() if it does not\n",
        "\n",
        "def query_processor(query):\n",
        "  matching_docx = set()\n",
        "  for word in word_tokenize(query):\n",
        "    word_lower = word.lower()\n",
        "    if word_lower not in SW and string.punctuation:\n",
        "      word_Lstem = Lstem.lemmatize(word_lower)\n",
        "      matches = invert_indx.get(word_Lstem)\n",
        "      if matches:\n",
        "       matching_docx |= matches\n",
        "      else:\n",
        "        print(\"Sorry, we got nothing.\")\n",
        "\n",
        "  return matching_docx"
      ],
      "metadata": {
        "id": "QiRGn3uxekNo"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing our search engine with inputs or \"queries\"\n",
        "#Testing with stopwords\n",
        "query_processor(\"ecg\")"
      ],
      "metadata": {
        "id": "51wriSM_j3HB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Ptitle[34]) #to display the titles corresponding to the output gotten from the processor\n",
        "#print(Ptitle[88])\n",
        "#print(Ptitle[187])"
      ],
      "metadata": {
        "id": "Gar09-b5lX51"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}